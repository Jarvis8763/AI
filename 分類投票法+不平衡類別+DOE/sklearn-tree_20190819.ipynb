{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\6506\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\sklearn\\tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %load tree.py\n",
    "\"\"\"\n",
    "This module gathers tree-based methods, including decision, regression and\n",
    "randomized trees. Single and multi-output problems are both handled.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Gilles Louppe <g.louppe@gmail.com>\n",
    "#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#          Brian Holt <bdholt1@gmail.com>\n",
    "#          Noel Dawe <noel@dawe.me>\n",
    "#          Satrajit Gosh <satrajit.ghosh@gmail.com>\n",
    "#          Joly Arnaud <arnaud.v.joly@gmail.com>\n",
    "#          Fares Hedayati <fares.hedayati@gmail.com>\n",
    "#          Nelson Liu <nelson@nelsonliu.me>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "\n",
    "import numbers\n",
    "import warnings\n",
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from ..base import BaseEstimator\n",
    "from ..base import ClassifierMixin\n",
    "from ..base import RegressorMixin\n",
    "from ..base import is_classifier\n",
    "from ..base import MultiOutputMixin\n",
    "from ..utils import check_array\n",
    "from ..utils import check_random_state\n",
    "from ..utils import compute_sample_weight\n",
    "from ..utils.multiclass import check_classification_targets\n",
    "from ..utils.validation import check_is_fitted\n",
    "\n",
    "from ._criterion import Criterion\n",
    "from ._splitter import Splitter\n",
    "from ._tree import DepthFirstTreeBuilder\n",
    "from ._tree import BestFirstTreeBuilder\n",
    "from ._tree import Tree\n",
    "from . import _tree, _splitter, _criterion\n",
    "\n",
    "__all__ = [\"DecisionTreeClassifier\",\n",
    "           \"DecisionTreeRegressor\",\n",
    "           \"ExtraTreeClassifier\",\n",
    "           \"ExtraTreeRegressor\"]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types and constants\n",
    "# =============================================================================\n",
    "\n",
    "DTYPE = _tree.DTYPE\n",
    "DOUBLE = _tree.DOUBLE\n",
    "\n",
    "CRITERIA_CLF = {\"gini\": _criterion.Gini, \"entropy\": _criterion.Entropy}\n",
    "CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE,\n",
    "                \"mae\": _criterion.MAE}\n",
    "\n",
    "DENSE_SPLITTERS = {\"best\": _splitter.BestSplitter,\n",
    "                   \"random\": _splitter.RandomSplitter}\n",
    "\n",
    "SPARSE_SPLITTERS = {\"best\": _splitter.BestSparseSplitter,\n",
    "                    \"random\": _splitter.RandomSparseSplitter}\n",
    "\n",
    "# =============================================================================\n",
    "# Base decision tree\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class BaseDecisionTree(BaseEstimator, MultiOutputMixin, metaclass=ABCMeta):\n",
    "    \"\"\"Base class for decision trees.\n",
    "\n",
    "    Warning: This class should not be used directly.\n",
    "    Use derived classes instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 criterion,\n",
    "                 splitter,\n",
    "                 max_depth,\n",
    "                 min_samples_split,\n",
    "                 min_samples_leaf,\n",
    "                 min_weight_fraction_leaf,\n",
    "                 max_features,\n",
    "                 max_leaf_nodes,\n",
    "                 random_state,\n",
    "                 min_impurity_decrease,\n",
    "                 min_impurity_split,\n",
    "                 class_weight=None,\n",
    "                 presort=False):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.class_weight = class_weight\n",
    "        self.presort = presort\n",
    "\n",
    "    def get_depth(self):\n",
    "        \"\"\"Returns the depth of the decision tree.\n",
    "\n",
    "        The depth of a tree is the maximum distance between the root\n",
    "        and any leaf.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'tree_')\n",
    "        return self.tree_.max_depth\n",
    "\n",
    "    def get_n_leaves(self):\n",
    "        \"\"\"Returns the number of leaves of the decision tree.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'tree_')\n",
    "        return self.tree_.n_leaves\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, check_input=True,\n",
    "            X_idx_sorted=None):\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        if check_input:\n",
    "            X = check_array(X, dtype=DTYPE, accept_sparse=\"csc\")\n",
    "            y = check_array(y, ensure_2d=False, dtype=None)\n",
    "            if issparse(X):\n",
    "                X.sort_indices()\n",
    "\n",
    "                if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:\n",
    "                    raise ValueError(\"No support for np.int64 index based \"\n",
    "                                     \"sparse matrices\")\n",
    "\n",
    "        # Determine output settings\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        is_classification = is_classifier(self)\n",
    "\n",
    "        y = np.atleast_1d(y)\n",
    "        expanded_class_weight = None\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            # reshape is necessary to preserve the data contiguity against vs\n",
    "            # [:, np.newaxis] that does not.\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "\n",
    "        self.n_outputs_ = y.shape[1]\n",
    "\n",
    "        if is_classification:\n",
    "            check_classification_targets(y)\n",
    "            y = np.copy(y)\n",
    "\n",
    "            self.classes_ = []\n",
    "            self.n_classes_ = []\n",
    "\n",
    "            if self.class_weight is not None:\n",
    "                y_original = np.copy(y)\n",
    "\n",
    "            y_encoded = np.zeros(y.shape, dtype=np.int)\n",
    "            for k in range(self.n_outputs_):\n",
    "                classes_k, y_encoded[:, k] = np.unique(y[:, k],\n",
    "                                                       return_inverse=True)\n",
    "                self.classes_.append(classes_k)\n",
    "                self.n_classes_.append(classes_k.shape[0])\n",
    "            y = y_encoded\n",
    "\n",
    "            if self.class_weight is not None:\n",
    "                expanded_class_weight = compute_sample_weight(\n",
    "                    self.class_weight, y_original)\n",
    "\n",
    "        else:\n",
    "            self.classes_ = [None] * self.n_outputs_\n",
    "            self.n_classes_ = [1] * self.n_outputs_\n",
    "\n",
    "        self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)\n",
    "\n",
    "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
    "            y = np.ascontiguousarray(y, dtype=DOUBLE)\n",
    "\n",
    "        # Check parameters\n",
    "        max_depth = ((2 ** 31) - 1 if self.max_depth is None\n",
    "                     else self.max_depth)\n",
    "        max_leaf_nodes = (-1 if self.max_leaf_nodes is None\n",
    "                          else self.max_leaf_nodes)\n",
    "\n",
    "        if isinstance(self.min_samples_leaf, (numbers.Integral, np.integer)):\n",
    "            if not 1 <= self.min_samples_leaf:\n",
    "                raise ValueError(\"min_samples_leaf must be at least 1 \"\n",
    "                                 \"or in (0, 0.5], got %s\"\n",
    "                                 % self.min_samples_leaf)\n",
    "            min_samples_leaf = self.min_samples_leaf\n",
    "        else:  # float\n",
    "            if not 0. < self.min_samples_leaf <= 0.5:\n",
    "                raise ValueError(\"min_samples_leaf must be at least 1 \"\n",
    "                                 \"or in (0, 0.5], got %s\"\n",
    "                                 % self.min_samples_leaf)\n",
    "            min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))\n",
    "\n",
    "        if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):\n",
    "            if not 2 <= self.min_samples_split:\n",
    "                raise ValueError(\"min_samples_split must be an integer \"\n",
    "                                 \"greater than 1 or a float in (0.0, 1.0]; \"\n",
    "                                 \"got the integer %s\"\n",
    "                                 % self.min_samples_split)\n",
    "            min_samples_split = self.min_samples_split\n",
    "        else:  # float\n",
    "            if not 0. < self.min_samples_split <= 1.:\n",
    "                raise ValueError(\"min_samples_split must be an integer \"\n",
    "                                 \"greater than 1 or a float in (0.0, 1.0]; \"\n",
    "                                 \"got the float %s\"\n",
    "                                 % self.min_samples_split)\n",
    "            min_samples_split = int(ceil(self.min_samples_split * n_samples))\n",
    "            min_samples_split = max(2, min_samples_split)\n",
    "\n",
    "        min_samples_split = max(min_samples_split, 2 * min_samples_leaf)\n",
    "\n",
    "        if isinstance(self.max_features, str):\n",
    "            if self.max_features == \"auto\":\n",
    "                if is_classification:\n",
    "                    max_features = max(1, int(np.sqrt(self.n_features_)))\n",
    "                else:\n",
    "                    max_features = self.n_features_\n",
    "            elif self.max_features == \"sqrt\":\n",
    "                max_features = max(1, int(np.sqrt(self.n_features_)))\n",
    "            elif self.max_features == \"log2\":\n",
    "                max_features = max(1, int(np.log2(self.n_features_)))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Invalid value for max_features. Allowed string '\n",
    "                    'values are \"auto\", \"sqrt\" or \"log2\".')\n",
    "        elif self.max_features is None:\n",
    "            max_features = self.n_features_\n",
    "        elif isinstance(self.max_features, (numbers.Integral, np.integer)):\n",
    "            max_features = self.max_features\n",
    "        else:  # float\n",
    "            if self.max_features > 0.0:\n",
    "                max_features = max(1,\n",
    "                                   int(self.max_features * self.n_features_))\n",
    "            else:\n",
    "                max_features = 0\n",
    "\n",
    "        self.max_features_ = max_features\n",
    "\n",
    "        if len(y) != n_samples:\n",
    "            raise ValueError(\"Number of labels=%d does not match \"\n",
    "                             \"number of samples=%d\" % (len(y), n_samples))\n",
    "        if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n",
    "            raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n",
    "        if max_depth <= 0:\n",
    "            raise ValueError(\"max_depth must be greater than zero. \")\n",
    "        if not (0 < max_features <= self.n_features_):\n",
    "            raise ValueError(\"max_features must be in (0, n_features]\")\n",
    "        if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n",
    "            raise ValueError(\"max_leaf_nodes must be integral number but was \"\n",
    "                             \"%r\" % max_leaf_nodes)\n",
    "        if -1 < max_leaf_nodes < 2:\n",
    "            raise ValueError((\"max_leaf_nodes {0} must be either None \"\n",
    "                              \"or larger than 1\").format(max_leaf_nodes))\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            if (getattr(sample_weight, \"dtype\", None) != DOUBLE or\n",
    "                    not sample_weight.flags.contiguous):\n",
    "                sample_weight = np.ascontiguousarray(\n",
    "                    sample_weight, dtype=DOUBLE)\n",
    "            if len(sample_weight.shape) > 1:\n",
    "                raise ValueError(\"Sample weights array has more \"\n",
    "                                 \"than one dimension: %d\" %\n",
    "                                 len(sample_weight.shape))\n",
    "            if len(sample_weight) != n_samples:\n",
    "                raise ValueError(\"Number of weights=%d does not match \"\n",
    "                                 \"number of samples=%d\" %\n",
    "                                 (len(sample_weight), n_samples))\n",
    "\n",
    "        if expanded_class_weight is not None:\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * expanded_class_weight\n",
    "            else:\n",
    "                sample_weight = expanded_class_weight\n",
    "\n",
    "        # Set min_weight_leaf from min_weight_fraction_leaf\n",
    "        if sample_weight is None:\n",
    "            min_weight_leaf = (self.min_weight_fraction_leaf *\n",
    "                               n_samples)\n",
    "        else:\n",
    "            min_weight_leaf = (self.min_weight_fraction_leaf *\n",
    "                               np.sum(sample_weight))\n",
    "\n",
    "        if self.min_impurity_split is not None:\n",
    "            warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
    "                          \"Its default value will change from 1e-7 to 0 in \"\n",
    "                          \"version 0.23, and it will be removed in 0.25. \"\n",
    "                          \"Use the min_impurity_decrease parameter instead.\",\n",
    "                          DeprecationWarning)\n",
    "            min_impurity_split = self.min_impurity_split\n",
    "        else:\n",
    "            min_impurity_split = 1e-7\n",
    "\n",
    "        if min_impurity_split < 0.:\n",
    "            raise ValueError(\"min_impurity_split must be greater than \"\n",
    "                             \"or equal to 0\")\n",
    "\n",
    "        if self.min_impurity_decrease < 0.:\n",
    "            raise ValueError(\"min_impurity_decrease must be greater than \"\n",
    "                             \"or equal to 0\")\n",
    "\n",
    "        allowed_presort = ('auto', True, False)\n",
    "        if self.presort not in allowed_presort:\n",
    "            raise ValueError(\"'presort' should be in {}. Got {!r} instead.\"\n",
    "                             .format(allowed_presort, self.presort))\n",
    "\n",
    "        if self.presort is True and issparse(X):\n",
    "            raise ValueError(\"Presorting is not supported for sparse \"\n",
    "                             \"matrices.\")\n",
    "\n",
    "        presort = self.presort\n",
    "        # Allow presort to be 'auto', which means True if the dataset is dense,\n",
    "        # otherwise it will be False.\n",
    "        if self.presort == 'auto':\n",
    "            presort = not issparse(X)\n",
    "\n",
    "        # If multiple trees are built on the same dataset, we only want to\n",
    "        # presort once. Splitters now can accept presorted indices if desired,\n",
    "        # but do not handle any presorting themselves. Ensemble algorithms\n",
    "        # which desire presorting must do presorting themselves and pass that\n",
    "        # matrix into each tree.\n",
    "        if X_idx_sorted is None and presort:\n",
    "            X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n",
    "                                             dtype=np.int32)\n",
    "\n",
    "        if presort and X_idx_sorted.shape != X.shape:\n",
    "            raise ValueError(\"The shape of X (X.shape = {}) doesn't match \"\n",
    "                             \"the shape of X_idx_sorted (X_idx_sorted\"\n",
    "                             \".shape = {})\".format(X.shape,\n",
    "                                                   X_idx_sorted.shape))\n",
    "\n",
    "        # Build tree\n",
    "        criterion = self.criterion\n",
    "        if not isinstance(criterion, Criterion):\n",
    "            if is_classification:\n",
    "                criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n",
    "                                                         self.n_classes_)\n",
    "            else:\n",
    "                criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
    "                                                         n_samples)\n",
    "\n",
    "        SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n",
    "\n",
    "        splitter = self.splitter\n",
    "        if not isinstance(self.splitter, Splitter):\n",
    "            splitter = SPLITTERS[self.splitter](criterion,\n",
    "                                                self.max_features_,\n",
    "                                                min_samples_leaf,\n",
    "                                                min_weight_leaf,\n",
    "                                                random_state,\n",
    "                                                self.presort)\n",
    "\n",
    "        self.tree_ = Tree(self.n_features_, self.n_classes_, self.n_outputs_)\n",
    "\n",
    "        # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise\n",
    "        if max_leaf_nodes < 0:\n",
    "            builder = DepthFirstTreeBuilder(splitter, min_samples_split,\n",
    "                                            min_samples_leaf,\n",
    "                                            min_weight_leaf,\n",
    "                                            max_depth,\n",
    "                                            self.min_impurity_decrease,\n",
    "                                            min_impurity_split)\n",
    "        else:\n",
    "            builder = BestFirstTreeBuilder(splitter, min_samples_split,\n",
    "                                           min_samples_leaf,\n",
    "                                           min_weight_leaf,\n",
    "                                           max_depth,\n",
    "                                           max_leaf_nodes,\n",
    "                                           self.min_impurity_decrease,\n",
    "                                           min_impurity_split)\n",
    "\n",
    "        builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
    "\n",
    "        if self.n_outputs_ == 1:\n",
    "            self.n_classes_ = self.n_classes_[0]\n",
    "            self.classes_ = self.classes_[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _validate_X_predict(self, X, check_input):\n",
    "        \"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\n",
    "        if check_input:\n",
    "            X = check_array(X, dtype=DTYPE, accept_sparse=\"csr\")\n",
    "            if issparse(X) and (X.indices.dtype != np.intc or\n",
    "                                X.indptr.dtype != np.intc):\n",
    "                raise ValueError(\"No support for np.int64 index based \"\n",
    "                                 \"sparse matrices\")\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        if self.n_features_ != n_features:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is %s and \"\n",
    "                             \"input n_features is %s \"\n",
    "                             % (self.n_features_, n_features))\n",
    "\n",
    "        return X\n",
    "\n",
    "    def predict(self, X, check_input=True):\n",
    "        \"\"\"Predict class or regression value for X.\n",
    "\n",
    "        For a classification model, the predicted class for each sample in X is\n",
    "        returned. For a regression model, the predicted value based on X is\n",
    "        returned.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "\n",
    "        check_input : boolean, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "            Don't use this parameter unless you know what you do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
    "            The predicted classes, or the predict values.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'tree_')\n",
    "        X = self._validate_X_predict(X, check_input)\n",
    "        proba = self.tree_.predict(X)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Classification\n",
    "        if is_classifier(self):\n",
    "            if self.n_outputs_ == 1:\n",
    "                return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n",
    "\n",
    "            else:\n",
    "                class_type = self.classes_[0].dtype\n",
    "                predictions = np.zeros((n_samples, self.n_outputs_),\n",
    "                                       dtype=class_type)\n",
    "                for k in range(self.n_outputs_):\n",
    "                    predictions[:, k] = self.classes_[k].take(\n",
    "                        np.argmax(proba[:, k], axis=1),\n",
    "                        axis=0)\n",
    "\n",
    "                return predictions\n",
    "\n",
    "        # Regression\n",
    "        else:\n",
    "            if self.n_outputs_ == 1:\n",
    "                return proba[:, 0]\n",
    "\n",
    "            else:\n",
    "                return proba[:, :, 0]\n",
    "\n",
    "    def apply(self, X, check_input=True):\n",
    "        \"\"\"\n",
    "        Returns the index of the leaf that each sample is predicted as.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "\n",
    "        check_input : boolean, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "            Don't use this parameter unless you know what you do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_leaves : array_like, shape = [n_samples,]\n",
    "            For each datapoint x in X, return the index of the leaf x\n",
    "            ends up in. Leaves are numbered within\n",
    "            ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
    "            numbering.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'tree_')\n",
    "        X = self._validate_X_predict(X, check_input)\n",
    "        return self.tree_.apply(X)\n",
    "\n",
    "    def decision_path(self, X, check_input=True):\n",
    "        \"\"\"Return the decision path in the tree\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "\n",
    "        check_input : boolean, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "            Don't use this parameter unless you know what you do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
    "            Return a node indicator matrix where non zero elements\n",
    "            indicates that the samples goes through the nodes.\n",
    "\n",
    "        \"\"\"\n",
    "        X = self._validate_X_predict(X, check_input)\n",
    "        return self.tree_.decision_path(X)\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"Return the feature importances.\n",
    "\n",
    "        The importance of a feature is computed as the (normalized) total\n",
    "        reduction of the criterion brought by that feature.\n",
    "        It is also known as the Gini importance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances_ : array, shape = [n_features]\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'tree_')\n",
    "\n",
    "        return self.tree_.compute_feature_importances()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Public estimators\n",
    "# =============================================================================\n",
    "\n",
    "class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):\n",
    "    \"\"\"A decision tree classifier.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criterion : string, optional (default=\"gini\")\n",
    "        The function to measure the quality of a split. Supported criteria are\n",
    "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "\n",
    "    splitter : string, optional (default=\"best\")\n",
    "        The strategy used to choose the split at each node. Supported\n",
    "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
    "        the best random split.\n",
    "\n",
    "    max_depth : int or None, optional (default=None)\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "\n",
    "    min_samples_split : int, float, optional (default=2)\n",
    "        The minimum number of samples required to split an internal node:\n",
    "\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_samples_leaf : int, float, optional (default=1)\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_weight_fraction_leaf : float, optional (default=0.)\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "\n",
    "    max_features : int, float, string or None, optional (default=None)\n",
    "        The number of features to consider when looking for the best split:\n",
    "\n",
    "            - If int, then consider `max_features` features at each split.\n",
    "            - If float, then `max_features` is a fraction and\n",
    "              `int(max_features * n_features)` features are considered at each\n",
    "              split.\n",
    "            - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "            - If \"log2\", then `max_features=log2(n_features)`.\n",
    "            - If None, then `max_features=n_features`.\n",
    "\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    max_leaf_nodes : int or None, optional (default=None)\n",
    "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "\n",
    "        The weighted impurity decrease equation is the following::\n",
    "\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "\n",
    "        .. versionadded:: 0.19\n",
    "\n",
    "    min_impurity_split : float, (default=1e-7)\n",
    "        Threshold for early stopping in tree growth. A node will split\n",
    "        if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "        .. deprecated:: 0.19\n",
    "           ``min_impurity_split`` has been deprecated in favor of\n",
    "           ``min_impurity_decrease`` in 0.19. The default value of\n",
    "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
    "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
    "\n",
    "    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one. For\n",
    "        multi-output problems, a list of dicts can be provided in the same\n",
    "        order as the columns of y.\n",
    "\n",
    "        Note that for multioutput (including multilabel) weights should be\n",
    "        defined for each class of every column in its own dict. For example,\n",
    "        for four-class multilabel classification weights should be\n",
    "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
    "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "\n",
    "    presort : bool, optional (default=False)\n",
    "        Whether to presort the data to speed up the finding of best splits in\n",
    "        fitting. For the default settings of a decision tree on large\n",
    "        datasets, setting this to true may slow down the training process.\n",
    "        When using either a smaller dataset or a restricted depth, this may\n",
    "        speed up the training.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] or a list of such arrays\n",
    "        The classes labels (single output problem),\n",
    "        or a list of arrays of class labels (multi-output problem).\n",
    "\n",
    "    feature_importances_ : array of shape = [n_features]\n",
    "        The feature importances. The higher, the more important the\n",
    "        feature. The importance of a feature is computed as the (normalized)\n",
    "        total reduction of the criterion brought by that feature.  It is also\n",
    "        known as the Gini importance [4]_.\n",
    "\n",
    "    max_features_ : int,\n",
    "        The inferred value of max_features.\n",
    "\n",
    "    n_classes_ : int or list\n",
    "        The number of classes (for single output problems),\n",
    "        or a list containing the number of classes for each\n",
    "        output (for multi-output problems).\n",
    "\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "\n",
    "    tree_ : Tree object\n",
    "        The underlying Tree object. Please refer to\n",
    "        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
    "        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
    "        for basic usage of these attributes.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "\n",
    "    The features are always randomly permuted at each split. Therefore,\n",
    "    the best found split may vary, even with the same training data and\n",
    "    ``max_features=n_features``, if the improvement of the criterion is\n",
    "    identical for several splits enumerated during the search of the best\n",
    "    split. To obtain a deterministic behaviour during fitting,\n",
    "    ``random_state`` has to be fixed.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    DecisionTreeRegressor\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
    "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
    "\n",
    "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
    "           Learning\", Springer, 2009.\n",
    "\n",
    "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
    "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> from sklearn.tree import DecisionTreeClassifier\n",
    "    >>> clf = DecisionTreeClassifier(random_state=0)\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "    ...                             # doctest: +SKIP\n",
    "    ...\n",
    "    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
    "            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 criterion=\"gini\",\n",
    "                 splitter=\"best\",\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=None,\n",
    "                 random_state=None,\n",
    "                 max_leaf_nodes=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None,\n",
    "                 class_weight=None,\n",
    "                 presort=False):\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            splitter=splitter,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_features=max_features,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            class_weight=class_weight,\n",
    "            random_state=random_state,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            presort=presort)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, check_input=True,\n",
    "            X_idx_sorted=None):\n",
    "        \"\"\"Build a decision tree classifier from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csc_matrix``.\n",
    "\n",
    "        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
    "            The target values (class labels) as integers or strings.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted. Splits\n",
    "            that would create child nodes with net zero or negative weight are\n",
    "            ignored while searching for a split in each node. Splits are also\n",
    "            ignored if they would result in any single class carrying a\n",
    "            negative weight in either child node.\n",
    "\n",
    "        check_input : boolean, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "            Don't use this parameter unless you know what you do.\n",
    "\n",
    "        X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
    "            The indexes of the sorted training input samples. If many tree\n",
    "            are grown on the same dataset, this allows the ordering to be\n",
    "            cached between trees. If None, the data will be sorted here.\n",
    "            Don't use this parameter unless you know what to do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        super().fit(\n",
    "            X, y,\n",
    "            sample_weight=sample_weight,\n",
    "            check_input=check_input,\n",
    "            X_idx_sorted=X_idx_sorted)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, check_input=True):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "\n",
    "        The predicted class probability is the fraction of samples of the same\n",
    "        class in a leaf.\n",
    "\n",
    "        check_input : boolean, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "            Don't use this parameter unless you know what you do.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "\n",
    "        check_input : bool\n",
    "            Run check_array on X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
    "            such arrays if n_outputs > 1.\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'tree_')\n",
    "        X = self._validate_X_predict(X, check_input)\n",
    "        proba = self.tree_.predict(X)\n",
    "\n",
    "        if self.n_outputs_ == 1:\n",
    "            proba = proba[:, :self.n_classes_]\n",
    "            normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "            normalizer[normalizer == 0.0] = 1.0\n",
    "            proba /= normalizer\n",
    "\n",
    "            return proba\n",
    "\n",
    "        else:\n",
    "            all_proba = []\n",
    "\n",
    "            for k in range(self.n_outputs_):\n",
    "                proba_k = proba[:, k, :self.n_classes_[k]]\n",
    "                normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
    "                normalizer[normalizer == 0.0] = 1.0\n",
    "                proba_k /= normalizer\n",
    "                all_proba.append(proba_k)\n",
    "\n",
    "            return all_proba\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict class log-probabilities of the input samples X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
    "            such arrays if n_outputs > 1.\n",
    "            The class log-probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "\n",
    "        if self.n_outputs_ == 1:\n",
    "            return np.log(proba)\n",
    "\n",
    "        else:\n",
    "            for k in range(self.n_outputs_):\n",
    "                proba[k] = np.log(proba[k])\n",
    "\n",
    "            return proba\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):\n",
    "    \"\"\"A decision tree regressor.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criterion : string, optional (default=\"mse\")\n",
    "        The function to measure the quality of a split. Supported criteria\n",
    "        are \"mse\" for the mean squared error, which is equal to variance\n",
    "        reduction as feature selection criterion and minimizes the L2 loss\n",
    "        using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
    "        squared error with Friedman's improvement score for potential splits,\n",
    "        and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
    "        using the median of each terminal node.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "           Mean Absolute Error (MAE) criterion.\n",
    "\n",
    "    splitter : string, optional (default=\"best\")\n",
    "        The strategy used to choose the split at each node. Supported\n",
    "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
    "        the best random split.\n",
    "\n",
    "    max_depth : int or None, optional (default=None)\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "\n",
    "    min_samples_split : int, float, optional (default=2)\n",
    "        The minimum number of samples required to split an internal node:\n",
    "\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_samples_leaf : int, float, optional (default=1)\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_weight_fraction_leaf : float, optional (default=0.)\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "\n",
    "    max_features : int, float, string or None, optional (default=None)\n",
    "        The number of features to consider when looking for the best split:\n",
    "\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    max_leaf_nodes : int or None, optional (default=None)\n",
    "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "\n",
    "        The weighted impurity decrease equation is the following::\n",
    "\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "\n",
    "        .. versionadded:: 0.19\n",
    "\n",
    "    min_impurity_split : float, (default=1e-7)\n",
    "        Threshold for early stopping in tree growth. A node will split\n",
    "        if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "        .. deprecated:: 0.19\n",
    "           ``min_impurity_split`` has been deprecated in favor of\n",
    "           ``min_impurity_decrease`` in 0.19. The default value of\n",
    "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
    "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
    "\n",
    "    presort : bool, optional (default=False)\n",
    "        Whether to presort the data to speed up the finding of best splits in\n",
    "        fitting. For the default settings of a decision tree on large\n",
    "        datasets, setting this to true may slow down the training process.\n",
    "        When using either a smaller dataset or a restricted depth, this may\n",
    "        speed up the training.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    feature_importances_ : array of shape = [n_features]\n",
    "        The feature importances.\n",
    "        The higher, the more important the feature.\n",
    "        The importance of a feature is computed as the\n",
    "        (normalized) total reduction of the criterion brought\n",
    "        by that feature. It is also known as the Gini importance [4]_.\n",
    "\n",
    "    max_features_ : int,\n",
    "        The inferred value of max_features.\n",
    "\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "\n",
    "    tree_ : Tree object\n",
    "        The underlying Tree object. Please refer to\n",
    "        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
    "        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
    "        for basic usage of these attributes.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "\n",
    "    The features are always randomly permuted at each split. Therefore,\n",
    "    the best found split may vary, even with the same training data and\n",
    "    ``max_features=n_features``, if the improvement of the criterion is\n",
    "    identical for several splits enumerated during the search of the best\n",
    "    split. To obtain a deterministic behaviour during fitting,\n",
    "    ``random_state`` has to be fixed.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    DecisionTreeClassifier\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
    "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
    "\n",
    "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
    "           Learning\", Springer, 2009.\n",
    "\n",
    "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
    "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_boston\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> from sklearn.tree import DecisionTreeRegressor\n",
    "    >>> boston = load_boston()\n",
    "    >>> regressor = DecisionTreeRegressor(random_state=0)\n",
    "    >>> cross_val_score(regressor, boston.data, boston.target, cv=10)\n",
    "    ...                    # doctest: +SKIP\n",
    "    ...\n",
    "    array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,\n",
    "            0.07..., 0.29..., 0.33..., -1.42..., -1.77...])\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 criterion=\"mse\",\n",
    "                 splitter=\"best\",\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=None,\n",
    "                 random_state=None,\n",
    "                 max_leaf_nodes=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None,\n",
    "                 presort=False):\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            splitter=splitter,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_features=max_features,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            random_state=random_state,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            presort=presort)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, check_input=True,\n",
    "            X_idx_sorted=None):\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csc_matrix``.\n",
    "\n",
    "        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
    "            The target values (real numbers). Use ``dtype=np.float64`` and\n",
    "            ``order='C'`` for maximum efficiency.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted. Splits\n",
    "            that would create child nodes with net zero or negative weight are\n",
    "            ignored while searching for a split in each node.\n",
    "\n",
    "        check_input : boolean, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "            Don't use this parameter unless you know what you do.\n",
    "\n",
    "        X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
    "            The indexes of the sorted training input samples. If many tree\n",
    "            are grown on the same dataset, this allows the ordering to be\n",
    "            cached between trees. If None, the data will be sorted here.\n",
    "            Don't use this parameter unless you know what to do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        super().fit(\n",
    "            X, y,\n",
    "            sample_weight=sample_weight,\n",
    "            check_input=check_input,\n",
    "            X_idx_sorted=X_idx_sorted)\n",
    "        return self\n",
    "\n",
    "\n",
    "class ExtraTreeClassifier(DecisionTreeClassifier):\n",
    "    \"\"\"An extremely randomized tree classifier.\n",
    "\n",
    "    Extra-trees differ from classic decision trees in the way they are built.\n",
    "    When looking for the best split to separate the samples of a node into two\n",
    "    groups, random splits are drawn for each of the `max_features` randomly\n",
    "    selected features and the best split among those is chosen. When\n",
    "    `max_features` is set 1, this amounts to building a totally random\n",
    "    decision tree.\n",
    "\n",
    "    Warning: Extra-trees should only be used within ensemble methods.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criterion : string, optional (default=\"gini\")\n",
    "        The function to measure the quality of a split. Supported criteria are\n",
    "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "\n",
    "    splitter : string, optional (default=\"random\")\n",
    "        The strategy used to choose the split at each node. Supported\n",
    "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
    "        the best random split.\n",
    "\n",
    "    max_depth : int or None, optional (default=None)\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "\n",
    "    min_samples_split : int, float, optional (default=2)\n",
    "        The minimum number of samples required to split an internal node:\n",
    "\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_samples_leaf : int, float, optional (default=1)\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_weight_fraction_leaf : float, optional (default=0.)\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "\n",
    "    max_features : int, float, string or None, optional (default=\"auto\")\n",
    "        The number of features to consider when looking for the best split:\n",
    "\n",
    "            - If int, then consider `max_features` features at each split.\n",
    "            - If float, then `max_features` is a fraction and\n",
    "              `int(max_features * n_features)` features are considered at each\n",
    "              split.\n",
    "            - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "            - If \"log2\", then `max_features=log2(n_features)`.\n",
    "            - If None, then `max_features=n_features`.\n",
    "\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    max_leaf_nodes : int or None, optional (default=None)\n",
    "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "\n",
    "        The weighted impurity decrease equation is the following::\n",
    "\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "\n",
    "        .. versionadded:: 0.19\n",
    "\n",
    "    min_impurity_split : float, (default=1e-7)\n",
    "        Threshold for early stopping in tree growth. A node will split\n",
    "        if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "        .. deprecated:: 0.19\n",
    "           ``min_impurity_split`` has been deprecated in favor of\n",
    "           ``min_impurity_decrease`` in 0.19. The default value of\n",
    "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
    "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
    "\n",
    "    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one. For\n",
    "        multi-output problems, a list of dicts can be provided in the same\n",
    "        order as the columns of y.\n",
    "\n",
    "        Note that for multioutput (including multilabel) weights should be\n",
    "        defined for each class of every column in its own dict. For example,\n",
    "        for four-class multilabel classification weights should be\n",
    "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
    "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    ExtraTreeRegressor, sklearn.ensemble.ExtraTreesClassifier,\n",
    "    sklearn.ensemble.ExtraTreesRegressor\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
    "           Machine Learning, 63(1), 3-42, 2006.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 criterion=\"gini\",\n",
    "                 splitter=\"random\",\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=\"auto\",\n",
    "                 random_state=None,\n",
    "                 max_leaf_nodes=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None,\n",
    "                 class_weight=None):\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            splitter=splitter,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_features=max_features,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            class_weight=class_weight,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            random_state=random_state)\n",
    "\n",
    "\n",
    "class ExtraTreeRegressor(DecisionTreeRegressor):\n",
    "    \"\"\"An extremely randomized tree regressor.\n",
    "\n",
    "    Extra-trees differ from classic decision trees in the way they are built.\n",
    "    When looking for the best split to separate the samples of a node into two\n",
    "    groups, random splits are drawn for each of the `max_features` randomly\n",
    "    selected features and the best split among those is chosen. When\n",
    "    `max_features` is set 1, this amounts to building a totally random\n",
    "    decision tree.\n",
    "\n",
    "    Warning: Extra-trees should only be used within ensemble methods.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criterion : string, optional (default=\"mse\")\n",
    "        The function to measure the quality of a split. Supported criteria\n",
    "        are \"mse\" for the mean squared error, which is equal to variance\n",
    "        reduction as feature selection criterion, and \"mae\" for the mean\n",
    "        absolute error.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "           Mean Absolute Error (MAE) criterion.\n",
    "\n",
    "    splitter : string, optional (default=\"random\")\n",
    "        The strategy used to choose the split at each node. Supported\n",
    "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
    "        the best random split.\n",
    "\n",
    "    max_depth : int or None, optional (default=None)\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "\n",
    "    min_samples_split : int, float, optional (default=2)\n",
    "        The minimum number of samples required to split an internal node:\n",
    "\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_samples_leaf : int, float, optional (default=1)\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "\n",
    "    min_weight_fraction_leaf : float, optional (default=0.)\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "\n",
    "    max_features : int, float, string or None, optional (default=\"auto\")\n",
    "        The number of features to consider when looking for the best split:\n",
    "\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "\n",
    "        The weighted impurity decrease equation is the following::\n",
    "\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "\n",
    "        .. versionadded:: 0.19\n",
    "\n",
    "    min_impurity_split : float, (default=1e-7)\n",
    "        Threshold for early stopping in tree growth. A node will split\n",
    "        if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "        .. deprecated:: 0.19\n",
    "           ``min_impurity_split`` has been deprecated in favor of\n",
    "           ``min_impurity_decrease`` in 0.19. The default value of\n",
    "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
    "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
    "\n",
    "    max_leaf_nodes : int or None, optional (default=None)\n",
    "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    ExtraTreeClassifier, sklearn.ensemble.ExtraTreesClassifier,\n",
    "    sklearn.ensemble.ExtraTreesRegressor\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
    "           Machine Learning, 63(1), 3-42, 2006.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 criterion=\"mse\",\n",
    "                 splitter=\"random\",\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=\"auto\",\n",
    "                 random_state=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None,\n",
    "                 max_leaf_nodes=None):\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            splitter=splitter,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_features=max_features,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %load _tree.pxd\n",
    "# Authors: Gilles Louppe <g.louppe@gmail.com>\n",
    "#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#          Brian Holt <bdholt1@gmail.com>\n",
    "#          Joel Nothman <joel.nothman@gmail.com>\n",
    "#          Arnaud Joly <arnaud.v.joly@gmail.com>\n",
    "#          Jacob Schreiber <jmschreiber91@gmail.com>\n",
    "#          Nelson Liu <nelson@nelsonliu.me>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# See _tree.pyx for details.\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "ctypedef np.npy_float32 DTYPE_t          # Type of X\n",
    "ctypedef np.npy_float64 DOUBLE_t         # Type of y, sample_weight\n",
    "ctypedef np.npy_intp SIZE_t              # Type for indices and counters\n",
    "ctypedef np.npy_int32 INT32_t            # Signed 32 bit integer\n",
    "ctypedef np.npy_uint32 UINT32_t          # Unsigned 32 bit integer\n",
    "\n",
    "from ._splitter cimport Splitter\n",
    "from ._splitter cimport SplitRecord\n",
    "\n",
    "cdef struct Node:\n",
    "    # Base storage structure for the nodes in a Tree object\n",
    "\n",
    "    SIZE_t left_child                    # id of the left child of the node\n",
    "    SIZE_t right_child                   # id of the right child of the node\n",
    "    SIZE_t feature                       # Feature used for splitting the node\n",
    "    DOUBLE_t threshold                   # Threshold value at the node\n",
    "    DOUBLE_t impurity                    # Impurity of the node (i.e., the value of the criterion)\n",
    "    SIZE_t n_node_samples                # Number of samples at the node\n",
    "    DOUBLE_t weighted_n_node_samples     # Weighted number of samples at the node\n",
    "\n",
    "\n",
    "cdef class Tree:\n",
    "    # The Tree object is a binary tree structure constructed by the\n",
    "    # TreeBuilder. The tree structure is used for predictions and\n",
    "    # feature importances.\n",
    "\n",
    "    # Input/Output layout\n",
    "    cdef public SIZE_t n_features        # Number of features in X\n",
    "    cdef SIZE_t* n_classes               # Number of classes in y[:, k]\n",
    "    cdef public SIZE_t n_outputs         # Number of outputs in y\n",
    "    cdef public SIZE_t max_n_classes     # max(n_classes)\n",
    "\n",
    "    # Inner structures: values are stored separately from node structure,\n",
    "    # since size is determined at runtime.\n",
    "    cdef public SIZE_t max_depth         # Max depth of the tree\n",
    "    cdef public SIZE_t node_count        # Counter for node IDs\n",
    "    cdef public SIZE_t capacity          # Capacity of tree, in terms of nodes\n",
    "    cdef Node* nodes                     # Array of nodes\n",
    "    cdef double* value                   # (capacity, n_outputs, max_n_classes) array of values\n",
    "    cdef SIZE_t value_stride             # = n_outputs * max_n_classes\n",
    "\n",
    "    # Methods\n",
    "    cdef SIZE_t _add_node(self, SIZE_t parent, bint is_left, bint is_leaf,\n",
    "                          SIZE_t feature, double threshold, double impurity,\n",
    "                          SIZE_t n_node_samples,\n",
    "                          double weighted_n_samples) nogil except -1\n",
    "    cdef int _resize(self, SIZE_t capacity) nogil except -1\n",
    "    cdef int _resize_c(self, SIZE_t capacity=*) nogil except -1\n",
    "\n",
    "    cdef np.ndarray _get_value_ndarray(self)\n",
    "    cdef np.ndarray _get_node_ndarray(self)\n",
    "\n",
    "    cpdef np.ndarray predict(self, object X)\n",
    "\n",
    "    cpdef np.ndarray apply(self, object X)\n",
    "    cdef np.ndarray _apply_dense(self, object X)\n",
    "    cdef np.ndarray _apply_sparse_csr(self, object X)\n",
    "\n",
    "    cpdef object decision_path(self, object X)\n",
    "    cdef object _decision_path_dense(self, object X)\n",
    "    cdef object _decision_path_sparse_csr(self, object X)\n",
    "\n",
    "    cpdef compute_feature_importances(self, normalize=*)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Tree builder\n",
    "# =============================================================================\n",
    "\n",
    "cdef class TreeBuilder:\n",
    "    # The TreeBuilder recursively builds a Tree object from training samples,\n",
    "    # using a Splitter object for splitting internal nodes and assigning\n",
    "    # values to leaves.\n",
    "    #\n",
    "    # This class controls the various stopping criteria and the node splitting\n",
    "    # evaluation order, e.g. depth-first or best-first.\n",
    "\n",
    "    cdef Splitter splitter              # Splitting algorithm\n",
    "\n",
    "    cdef SIZE_t min_samples_split       # Minimum number of samples in an internal node\n",
    "    cdef SIZE_t min_samples_leaf        # Minimum number of samples in a leaf\n",
    "    cdef double min_weight_leaf         # Minimum weight in a leaf\n",
    "    cdef SIZE_t max_depth               # Maximal tree depth\n",
    "    cdef double min_impurity_split\n",
    "    cdef double min_impurity_decrease   # Impurity threshold for early stopping\n",
    "\n",
    "    cpdef build(self, Tree tree, object X, np.ndarray y,\n",
    "                np.ndarray sample_weight=*,\n",
    "                np.ndarray X_idx_sorted=*)\n",
    "    cdef _check_input(self, object X, np.ndarray y, np.ndarray sample_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting export.py\n"
     ]
    }
   ],
   "source": [
    "%%file export.py\n",
    "\"\"\"\n",
    "This module defines export functions for decision trees.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Gilles Louppe <g.louppe@gmail.com>\n",
    "#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#          Brian Holt <bdholt1@gmail.com>\n",
    "#          Noel Dawe <noel@dawe.me>\n",
    "#          Satrajit Gosh <satrajit.ghosh@gmail.com>\n",
    "#          Trevor Stephens <trev.stephens@gmail.com>\n",
    "#          Li Li <aiki.nogard@gmail.com>\n",
    "#          Giuseppe Vettigli <vettigli@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "from numbers import Integral\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ..utils.validation import check_is_fitted\n",
    "\n",
    "from . import _criterion\n",
    "from . import _tree\n",
    "from ._reingold_tilford import buchheim, Tree\n",
    "from . import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def _color_brew(n):\n",
    "    \"\"\"Generate n colors with equally spaced hues.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        The number of colors required.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    color_list : list, length n\n",
    "        List of n tuples of form (R, G, B) being the components of each color.\n",
    "    \"\"\"\n",
    "    color_list = []\n",
    "\n",
    "    # Initialize saturation & value; calculate chroma & value shift\n",
    "    s, v = 0.75, 0.9\n",
    "    c = s * v\n",
    "    m = v - c\n",
    "\n",
    "    for h in np.arange(25, 385, 360. / n).astype(int):\n",
    "        # Calculate some intermediate values\n",
    "        h_bar = h / 60.\n",
    "        x = c * (1 - abs((h_bar % 2) - 1))\n",
    "        # Initialize RGB with same hue & chroma as our color\n",
    "        rgb = [(c, x, 0),\n",
    "               (x, c, 0),\n",
    "               (0, c, x),\n",
    "               (0, x, c),\n",
    "               (x, 0, c),\n",
    "               (c, 0, x),\n",
    "               (c, x, 0)]\n",
    "        r, g, b = rgb[int(h_bar)]\n",
    "        # Shift the initial RGB values to match value and store\n",
    "        rgb = [(int(255 * (r + m))),\n",
    "               (int(255 * (g + m))),\n",
    "               (int(255 * (b + m)))]\n",
    "        color_list.append(rgb)\n",
    "\n",
    "    return color_list\n",
    "\n",
    "\n",
    "class Sentinel(object):\n",
    "    def __repr__(self):\n",
    "        return '\"tree.dot\"'\n",
    "\n",
    "\n",
    "SENTINEL = Sentinel()\n",
    "\n",
    "\n",
    "def plot_tree(decision_tree, max_depth=None, feature_names=None,\n",
    "              class_names=None, label='all', filled=False,\n",
    "              impurity=True, node_ids=False,\n",
    "              proportion=False, rotate=False, rounded=False,\n",
    "              precision=3, ax=None, fontsize=None):\n",
    "    \"\"\"Plot a decision tree.\n",
    "\n",
    "    The sample counts that are shown are weighted with any sample_weights that\n",
    "    might be present.\n",
    "    This function requires matplotlib, and works best with matplotlib >= 1.5.\n",
    "\n",
    "    The visualization is fit automatically to the size of the axis.\n",
    "    Use the ``figsize`` or ``dpi`` arguments of ``plt.figure``  to control\n",
    "    the size of the rendering.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    .. versionadded:: 0.21\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree : decision tree regressor or classifier\n",
    "        The decision tree to be exported to GraphViz.\n",
    "\n",
    "    max_depth : int, optional (default=None)\n",
    "        The maximum depth of the representation. If None, the tree is fully\n",
    "        generated.\n",
    "\n",
    "    feature_names : list of strings, optional (default=None)\n",
    "        Names of each of the features.\n",
    "\n",
    "    class_names : list of strings, bool or None, optional (default=None)\n",
    "        Names of each of the target classes in ascending numerical order.\n",
    "        Only relevant for classification and not supported for multi-output.\n",
    "        If ``True``, shows a symbolic representation of the class name.\n",
    "\n",
    "    label : {'all', 'root', 'none'}, optional (default='all')\n",
    "        Whether to show informative labels for impurity, etc.\n",
    "        Options include 'all' to show at every node, 'root' to show only at\n",
    "        the top root node, or 'none' to not show at any node.\n",
    "\n",
    "    filled : bool, optional (default=False)\n",
    "        When set to ``True``, paint nodes to indicate majority class for\n",
    "        classification, extremity of values for regression, or purity of node\n",
    "        for multi-output.\n",
    "\n",
    "    impurity : bool, optional (default=True)\n",
    "        When set to ``True``, show the impurity at each node.\n",
    "\n",
    "    node_ids : bool, optional (default=False)\n",
    "        When set to ``True``, show the ID number on each node.\n",
    "\n",
    "    proportion : bool, optional (default=False)\n",
    "        When set to ``True``, change the display of 'values' and/or 'samples'\n",
    "        to be proportions and percentages respectively.\n",
    "\n",
    "    rotate : bool, optional (default=False)\n",
    "        When set to ``True``, orient tree left to right rather than top-down.\n",
    "\n",
    "    rounded : bool, optional (default=False)\n",
    "        When set to ``True``, draw node boxes with rounded corners and use\n",
    "        Helvetica fonts instead of Times-Roman.\n",
    "\n",
    "    precision : int, optional (default=3)\n",
    "        Number of digits of precision for floating point in the values of\n",
    "        impurity, threshold and value attributes of each node.\n",
    "\n",
    "    ax : matplotlib axis, optional (default=None)\n",
    "        Axes to plot to. If None, use current axis. Any previous content\n",
    "        is cleared.\n",
    "\n",
    "    fontsize : int, optional (default=None)\n",
    "        Size of text font. If None, determined automatically to fit figure.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    annotations : list of artists\n",
    "        List containing the artists for the annotation boxes making up the\n",
    "        tree.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn import tree\n",
    "\n",
    "    >>> clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "    >>> iris = load_iris()\n",
    "\n",
    "    >>> clf = clf.fit(iris.data, iris.target)\n",
    "    >>> tree.plot_tree(clf)  # doctest: +SKIP\n",
    "    [Text(251.5,345.217,'X[3] <= 0.8...\n",
    "\n",
    "    \"\"\"\n",
    "    exporter = _MPLTreeExporter(\n",
    "        max_depth=max_depth, feature_names=feature_names,\n",
    "        class_names=class_names, label=label, filled=filled,\n",
    "        impurity=impurity, node_ids=node_ids,\n",
    "        proportion=proportion, rotate=rotate, rounded=rounded,\n",
    "        precision=precision, fontsize=fontsize)\n",
    "    return exporter.export(decision_tree, ax=ax)\n",
    "\n",
    "\n",
    "class _BaseTreeExporter(object):\n",
    "    def __init__(self, max_depth=None, feature_names=None,\n",
    "                 class_names=None, label='all', filled=False,\n",
    "                 impurity=True, node_ids=False,\n",
    "                 proportion=False, rotate=False, rounded=False,\n",
    "                 precision=3, fontsize=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_names = feature_names\n",
    "        self.class_names = class_names\n",
    "        self.label = label\n",
    "        self.filled = filled\n",
    "        self.impurity = impurity\n",
    "        self.node_ids = node_ids\n",
    "        self.proportion = proportion\n",
    "        self.rotate = rotate\n",
    "        self.rounded = rounded\n",
    "        self.precision = precision\n",
    "        self.fontsize = fontsize\n",
    "\n",
    "    def get_color(self, value):\n",
    "        # Find the appropriate color & intensity for a node\n",
    "        if self.colors['bounds'] is None:\n",
    "            # Classification tree\n",
    "            color = list(self.colors['rgb'][np.argmax(value)])\n",
    "            sorted_values = sorted(value, reverse=True)\n",
    "            if len(sorted_values) == 1:\n",
    "                alpha = 0\n",
    "            else:\n",
    "                alpha = ((sorted_values[0] - sorted_values[1])\n",
    "                         / (1 - sorted_values[1]))\n",
    "        else:\n",
    "            # Regression tree or multi-output\n",
    "            color = list(self.colors['rgb'][0])\n",
    "            alpha = ((value - self.colors['bounds'][0]) /\n",
    "                     (self.colors['bounds'][1] - self.colors['bounds'][0]))\n",
    "        # unpack numpy scalars\n",
    "        alpha = float(alpha)\n",
    "        # compute the color as alpha against white\n",
    "        color = [int(round(alpha * c + (1 - alpha) * 255, 0)) for c in color]\n",
    "        # Return html color code in #RRGGBB format\n",
    "        return '#%2x%2x%2x' % tuple(color)\n",
    "\n",
    "    def get_fill_color(self, tree, node_id):\n",
    "        # Fetch appropriate color for node\n",
    "        if 'rgb' not in self.colors:\n",
    "            # Initialize colors and bounds if required\n",
    "            self.colors['rgb'] = _color_brew(tree.n_classes[0])\n",
    "            if tree.n_outputs != 1:\n",
    "                # Find max and min impurities for multi-output\n",
    "                self.colors['bounds'] = (np.min(-tree.impurity),\n",
    "                                         np.max(-tree.impurity))\n",
    "            elif (tree.n_classes[0] == 1 and\n",
    "                  len(np.unique(tree.value)) != 1):\n",
    "                # Find max and min values in leaf nodes for regression\n",
    "                self.colors['bounds'] = (np.min(tree.value),\n",
    "                                         np.max(tree.value))\n",
    "        if tree.n_outputs == 1:\n",
    "            node_val = (tree.value[node_id][0, :] /\n",
    "                        tree.weighted_n_node_samples[node_id])\n",
    "            if tree.n_classes[0] == 1:\n",
    "                # Regression\n",
    "                node_val = tree.value[node_id][0, :]\n",
    "        else:\n",
    "            # If multi-output color node by impurity\n",
    "            node_val = -tree.impurity[node_id]\n",
    "        return self.get_color(node_val)\n",
    "\n",
    "    def node_to_str(self, tree, node_id, criterion):\n",
    "        # Generate the node content string\n",
    "        if tree.n_outputs == 1:\n",
    "            value = tree.value[node_id][0, :]\n",
    "        else:\n",
    "            value = tree.value[node_id]\n",
    "\n",
    "        # Should labels be shown?\n",
    "        labels = (self.label == 'root' and node_id == 0) or self.label == 'all'\n",
    "\n",
    "        characters = self.characters\n",
    "        node_string = characters[-1]\n",
    "\n",
    "        # Write node ID\n",
    "        if self.node_ids:\n",
    "            if labels:\n",
    "                node_string += 'node '\n",
    "            node_string += characters[0] + str(node_id) + characters[4]\n",
    "\n",
    "        # Write decision criteria\n",
    "        if tree.children_left[node_id] != _tree.TREE_LEAF:\n",
    "            # Always write node decision criteria, except for leaves\n",
    "            if self.feature_names is not None:\n",
    "                feature = self.feature_names[tree.feature[node_id]]\n",
    "            else:\n",
    "                feature = \"X%s%s%s\" % (characters[1],\n",
    "                                       tree.feature[node_id],\n",
    "                                       characters[2])\n",
    "#--------------------------- \n",
    "\n",
    "            import pandas as pd            \n",
    "            cat=pd.read_csv('C:/Users/6506/Desktop/JL/變數刪除後123/分析大表training_data_11_flted.csv',encoding='ANSI').iloc[:,2:]\n",
    "            null_columns=cat.columns[cat.isnull().any()]\n",
    "            cat.loc[cat.isnull().any(axis=1),null_columns]='NA'\n",
    "            \n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            from collections import defaultdict\n",
    "            d = defaultdict(LabelEncoder)\n",
    "            cat.apply(lambda x: d[x.name].fit(x))\n",
    "            \n",
    "            if cat[feature].dtypes ==object:\n",
    "                node_string += '%s %s %s%s' % (feature,\n",
    "                                           '=',\n",
    "                                           ', '.join(d[feature].classes_[0:int(tree.threshold[node_id])+1]),\n",
    "                                           characters[4])\n",
    "            else:\n",
    "                node_string += '%s %s %s%s' % (feature,\n",
    "                                           characters[3],\n",
    "                                           round(tree.threshold[node_id],self.precision),\n",
    "                                           characters[4])\n",
    "#-----------------------------\n",
    "        # Write impurity\n",
    "        if self.impurity:\n",
    "            if isinstance(criterion, _criterion.FriedmanMSE):\n",
    "                criterion = \"friedman_mse\"\n",
    "            elif not isinstance(criterion, str):\n",
    "                criterion = \"impurity\"\n",
    "            if labels:\n",
    "                node_string += '%s = ' % criterion\n",
    "            node_string += (str(round(tree.impurity[node_id], self.precision))\n",
    "                            + characters[4])\n",
    "\n",
    "        # Write node sample count\n",
    "        if labels:\n",
    "            node_string += 'samples = '\n",
    "        if self.proportion:\n",
    "            percent = (100. * tree.n_node_samples[node_id] /\n",
    "                       float(tree.n_node_samples[0]))\n",
    "            node_string += (str(round(percent, 1)) + '%' +\n",
    "                            characters[4])\n",
    "        else:\n",
    "            node_string += (str(tree.n_node_samples[node_id]) +\n",
    "                            characters[4])\n",
    "\n",
    "        # Write node class distribution / regression value\n",
    "        if self.proportion and tree.n_classes[0] != 1:\n",
    "            # For classification this will show the proportion of samples\n",
    "            value = value / tree.weighted_n_node_samples[node_id]\n",
    "        if labels:\n",
    "            node_string += 'value = '\n",
    "        if tree.n_classes[0] == 1:\n",
    "            # Regression\n",
    "            value_text = np.around(value, self.precision)\n",
    "        elif self.proportion:\n",
    "            # Classification\n",
    "            value_text = np.around(value, self.precision)\n",
    "        elif np.all(np.equal(np.mod(value, 1), 0)):\n",
    "            # Classification without floating-point weights\n",
    "            value_text = value.astype(int)\n",
    "        else:\n",
    "            # Classification with floating-point weights\n",
    "            value_text = np.around(value, self.precision)\n",
    "        # Strip whitespace\n",
    "        value_text = str(value_text.astype('S32')).replace(\"b'\", \"'\")\n",
    "        value_text = value_text.replace(\"' '\", \", \").replace(\"'\", \"\")\n",
    "        if tree.n_classes[0] == 1 and tree.n_outputs == 1:\n",
    "            value_text = value_text.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        value_text = value_text.replace(\"\\n \", characters[4])\n",
    "        node_string += value_text + characters[4]\n",
    "\n",
    "        # Write node majority class\n",
    "        if (self.class_names is not None and\n",
    "                tree.n_classes[0] != 1 and\n",
    "                tree.n_outputs == 1):\n",
    "            # Only done for single-output classification trees\n",
    "            if labels:\n",
    "                node_string += 'class = '\n",
    "            if self.class_names is not True:\n",
    "                class_name = self.class_names[np.argmax(value)]\n",
    "            else:\n",
    "                class_name = \"y%s%s%s\" % (characters[1],\n",
    "                                          np.argmax(value),\n",
    "                                          characters[2])\n",
    "            node_string += class_name\n",
    "\n",
    "        # Clean up any trailing newlines\n",
    "        if node_string.endswith(characters[4]):\n",
    "            node_string = node_string[:-len(characters[4])]\n",
    "\n",
    "        return node_string + characters[5]\n",
    "\n",
    "\n",
    "class _DOTTreeExporter(_BaseTreeExporter):\n",
    "    def __init__(self, out_file=SENTINEL, max_depth=None,\n",
    "                 feature_names=None, class_names=None, label='all',\n",
    "                 filled=False, leaves_parallel=False, impurity=True,\n",
    "                 node_ids=False, proportion=False, rotate=False, rounded=False,\n",
    "                 special_characters=False, precision=3):\n",
    "\n",
    "        super().__init__(\n",
    "            max_depth=max_depth, feature_names=feature_names,\n",
    "            class_names=class_names, label=label, filled=filled,\n",
    "            impurity=impurity,\n",
    "            node_ids=node_ids, proportion=proportion, rotate=rotate,\n",
    "            rounded=rounded,\n",
    "            precision=precision)\n",
    "        self.leaves_parallel = leaves_parallel\n",
    "        self.out_file = out_file\n",
    "        self.special_characters = special_characters\n",
    "\n",
    "        # PostScript compatibility for special characters\n",
    "        if special_characters:\n",
    "            self.characters = ['&#35;', '<SUB>', '</SUB>', '&le;', '<br/>',\n",
    "                               '>', '<']\n",
    "        else:\n",
    "            self.characters = ['#', '[', ']', '<=', '\\\\n', '\"', '\"']\n",
    "\n",
    "        # validate\n",
    "        if isinstance(precision, Integral):\n",
    "            if precision < 0:\n",
    "                raise ValueError(\"'precision' should be greater or equal to 0.\"\n",
    "                                 \" Got {} instead.\".format(precision))\n",
    "        else:\n",
    "            raise ValueError(\"'precision' should be an integer. Got {}\"\n",
    "                             \" instead.\".format(type(precision)))\n",
    "\n",
    "        # The depth of each node for plotting with 'leaf' option\n",
    "        self.ranks = {'leaves': []}\n",
    "        # The colors to render each node with\n",
    "        self.colors = {'bounds': None}\n",
    "\n",
    "    def export(self, decision_tree):\n",
    "        # Check length of feature_names before getting into the tree node\n",
    "        # Raise error if length of feature_names does not match\n",
    "        # n_features_ in the decision_tree\n",
    "        if self.feature_names is not None:\n",
    "            if len(self.feature_names) != decision_tree.n_features_:\n",
    "                raise ValueError(\"Length of feature_names, %d \"\n",
    "                                 \"does not match number of features, %d\"\n",
    "                                 % (len(self.feature_names),\n",
    "                                    decision_tree.n_features_))\n",
    "        # each part writes to out_file\n",
    "        self.head()\n",
    "        # Now recurse the tree and add node & edge attributes\n",
    "        if isinstance(decision_tree, _tree.Tree):\n",
    "            self.recurse(decision_tree, 0, criterion=\"impurity\")\n",
    "        else:\n",
    "            self.recurse(decision_tree.tree_, 0,\n",
    "                         criterion=decision_tree.criterion)\n",
    "\n",
    "        self.tail()\n",
    "\n",
    "    def tail(self):\n",
    "        # If required, draw leaf nodes at same depth as each other\n",
    "        if self.leaves_parallel:\n",
    "            for rank in sorted(self.ranks):\n",
    "                self.out_file.write(\n",
    "                    \"{rank=same ; \" +\n",
    "                    \"; \".join(r for r in self.ranks[rank]) + \"} ;\\n\")\n",
    "        self.out_file.write(\"}\")\n",
    "\n",
    "    def head(self):\n",
    "        self.out_file.write('digraph Tree {\\n')\n",
    "\n",
    "        # Specify node aesthetics\n",
    "        self.out_file.write('node [shape=box')\n",
    "        rounded_filled = []\n",
    "        if self.filled:\n",
    "            rounded_filled.append('filled')\n",
    "        if self.rounded:\n",
    "            rounded_filled.append('rounded')\n",
    "        if len(rounded_filled) > 0:\n",
    "            self.out_file.write(\n",
    "                ', style=\"%s\", color=\"black\"'\n",
    "                % \", \".join(rounded_filled))\n",
    "        if self.rounded:\n",
    "            self.out_file.write(', fontname=helvetica')\n",
    "        self.out_file.write('] ;\\n')\n",
    "\n",
    "        # Specify graph & edge aesthetics\n",
    "        if self.leaves_parallel:\n",
    "            self.out_file.write(\n",
    "                'graph [ranksep=equally, splines=polyline] ;\\n')\n",
    "        if self.rounded:\n",
    "            self.out_file.write('edge [fontname=helvetica] ;\\n')\n",
    "        if self.rotate:\n",
    "            self.out_file.write('rankdir=LR ;\\n')\n",
    "\n",
    "    def recurse(self, tree, node_id, criterion, parent=None, depth=0):\n",
    "        if node_id == _tree.TREE_LEAF:\n",
    "            raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)\n",
    "\n",
    "        left_child = tree.children_left[node_id]\n",
    "        right_child = tree.children_right[node_id]\n",
    "\n",
    "        # Add node with description\n",
    "        if self.max_depth is None or depth <= self.max_depth:\n",
    "\n",
    "            # Collect ranks for 'leaf' option in plot_options\n",
    "            if left_child == _tree.TREE_LEAF:\n",
    "                self.ranks['leaves'].append(str(node_id))\n",
    "            elif str(depth) not in self.ranks:\n",
    "                self.ranks[str(depth)] = [str(node_id)]\n",
    "            else:\n",
    "                self.ranks[str(depth)].append(str(node_id))\n",
    "\n",
    "            self.out_file.write(\n",
    "                '%d [label=%s' % (node_id, self.node_to_str(tree, node_id,\n",
    "                                                            criterion)))\n",
    "\n",
    "            if self.filled:\n",
    "                self.out_file.write(', fillcolor=\"%s\"'\n",
    "                                    % self.get_fill_color(tree, node_id))\n",
    "            self.out_file.write('] ;\\n')\n",
    "\n",
    "            if parent is not None:\n",
    "                # Add edge to parent\n",
    "                self.out_file.write('%d -> %d' % (parent, node_id))\n",
    "                if parent == 0:\n",
    "                    # Draw True/False labels if parent is root node\n",
    "                    angles = np.array([45, -45]) * ((self.rotate - .5) * -2)\n",
    "                    self.out_file.write(' [labeldistance=2.5, labelangle=')\n",
    "                    if node_id == 1:\n",
    "                        self.out_file.write('%d, headlabel=\"True\"]' %\n",
    "                                            angles[0])\n",
    "                    else:\n",
    "                        self.out_file.write('%d, headlabel=\"False\"]' %\n",
    "                                            angles[1])\n",
    "                self.out_file.write(' ;\\n')\n",
    "\n",
    "            if left_child != _tree.TREE_LEAF:\n",
    "                self.recurse(tree, left_child, criterion=criterion,\n",
    "                             parent=node_id, depth=depth + 1)\n",
    "                self.recurse(tree, right_child, criterion=criterion,\n",
    "                             parent=node_id, depth=depth + 1)\n",
    "\n",
    "        else:\n",
    "            self.ranks['leaves'].append(str(node_id))\n",
    "\n",
    "            self.out_file.write('%d [label=\"(...)\"' % node_id)\n",
    "            if self.filled:\n",
    "                # color cropped nodes grey\n",
    "                self.out_file.write(', fillcolor=\"#C0C0C0\"')\n",
    "            self.out_file.write('] ;\\n' % node_id)\n",
    "\n",
    "            if parent is not None:\n",
    "                # Add edge to parent\n",
    "                self.out_file.write('%d -> %d ;\\n' % (parent, node_id))\n",
    "\n",
    "\n",
    "class _MPLTreeExporter(_BaseTreeExporter):\n",
    "    def __init__(self, max_depth=None, feature_names=None,\n",
    "                 class_names=None, label='all', filled=False,\n",
    "                 impurity=True, node_ids=False,\n",
    "                 proportion=False, rotate=False, rounded=False,\n",
    "                 precision=3, fontsize=None):\n",
    "\n",
    "        super().__init__(\n",
    "            max_depth=max_depth, feature_names=feature_names,\n",
    "            class_names=class_names, label=label, filled=filled,\n",
    "            impurity=impurity, node_ids=node_ids, proportion=proportion,\n",
    "            rotate=rotate, rounded=rounded, precision=precision)\n",
    "        self.fontsize = fontsize\n",
    "\n",
    "        # validate\n",
    "        if isinstance(precision, Integral):\n",
    "            if precision < 0:\n",
    "                raise ValueError(\"'precision' should be greater or equal to 0.\"\n",
    "                                 \" Got {} instead.\".format(precision))\n",
    "        else:\n",
    "            raise ValueError(\"'precision' should be an integer. Got {}\"\n",
    "                             \" instead.\".format(type(precision)))\n",
    "\n",
    "        # The depth of each node for plotting with 'leaf' option\n",
    "        self.ranks = {'leaves': []}\n",
    "        # The colors to render each node with\n",
    "        self.colors = {'bounds': None}\n",
    "\n",
    "        self.characters = ['#', '[', ']', '<=', '\\n', '', '']\n",
    "\n",
    "        self.bbox_args = dict(fc='w')\n",
    "        if self.rounded:\n",
    "            self.bbox_args['boxstyle'] = \"round\"\n",
    "        else:\n",
    "            # matplotlib <1.5 requires explicit boxstyle\n",
    "            self.bbox_args['boxstyle'] = \"square\"\n",
    "\n",
    "        self.arrow_args = dict(arrowstyle=\"<-\")\n",
    "\n",
    "    def _make_tree(self, node_id, et, depth=0):\n",
    "        # traverses _tree.Tree recursively, builds intermediate\n",
    "        # \"_reingold_tilford.Tree\" object\n",
    "        name = self.node_to_str(et, node_id, criterion='entropy')\n",
    "        if (et.children_left[node_id] != _tree.TREE_LEAF\n",
    "                and (self.max_depth is None or depth <= self.max_depth)):\n",
    "            children = [self._make_tree(et.children_left[node_id], et,\n",
    "                                        depth=depth + 1),\n",
    "                        self._make_tree(et.children_right[node_id], et,\n",
    "                                        depth=depth + 1)]\n",
    "        else:\n",
    "            return Tree(name, node_id)\n",
    "        return Tree(name, node_id, *children)\n",
    "\n",
    "    def export(self, decision_tree, ax=None):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.text import Annotation\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        ax.clear()\n",
    "        ax.set_axis_off()\n",
    "        my_tree = self._make_tree(0, decision_tree.tree_)\n",
    "        draw_tree = buchheim(my_tree)\n",
    "\n",
    "        # important to make sure we're still\n",
    "        # inside the axis after drawing the box\n",
    "        # this makes sense because the width of a box\n",
    "        # is about the same as the distance between boxes\n",
    "        max_x, max_y = draw_tree.max_extents() + 1\n",
    "        ax_width = ax.get_window_extent().width\n",
    "        ax_height = ax.get_window_extent().height\n",
    "\n",
    "        scale_x = ax_width / max_x\n",
    "        scale_y = ax_height / max_y\n",
    "\n",
    "        self.recurse(draw_tree, decision_tree.tree_, ax,\n",
    "                     scale_x, scale_y, ax_height)\n",
    "\n",
    "        anns = [ann for ann in ax.get_children()\n",
    "                if isinstance(ann, Annotation)]\n",
    "\n",
    "        # update sizes of all bboxes\n",
    "        renderer = ax.figure.canvas.get_renderer()\n",
    "\n",
    "        for ann in anns:\n",
    "            ann.update_bbox_position_size(renderer)\n",
    "\n",
    "        if self.fontsize is None:\n",
    "            # get figure to data transform\n",
    "            # adjust fontsize to avoid overlap\n",
    "            # get max box width and height\n",
    "            try:\n",
    "                extents = [ann.get_bbox_patch().get_window_extent()\n",
    "                           for ann in anns]\n",
    "                max_width = max([extent.width for extent in extents])\n",
    "                max_height = max([extent.height for extent in extents])\n",
    "                # width should be around scale_x in axis coordinates\n",
    "                size = anns[0].get_fontsize() * min(scale_x / max_width,\n",
    "                                                    scale_y / max_height)\n",
    "                for ann in anns:\n",
    "                    ann.set_fontsize(size)\n",
    "            except AttributeError:\n",
    "                # matplotlib < 1.5\n",
    "                warnings.warn(\"Automatic scaling of tree plots requires \"\n",
    "                              \"matplotlib 1.5 or higher. Please specify \"\n",
    "                              \"fontsize.\")\n",
    "\n",
    "        return anns\n",
    "\n",
    "    def recurse(self, node, tree, ax, scale_x, scale_y, height, depth=0):\n",
    "        # need to copy bbox args because matplotib <1.5 modifies them\n",
    "        kwargs = dict(bbox=self.bbox_args.copy(), ha='center', va='center',\n",
    "                      zorder=100 - 10 * depth, xycoords='axes pixels')\n",
    "\n",
    "        if self.fontsize is not None:\n",
    "            kwargs['fontsize'] = self.fontsize\n",
    "\n",
    "        # offset things by .5 to center them in plot\n",
    "        xy = ((node.x + .5) * scale_x, height - (node.y + .5) * scale_y)\n",
    "\n",
    "        if self.max_depth is None or depth <= self.max_depth:\n",
    "            if self.filled:\n",
    "                kwargs['bbox']['fc'] = self.get_fill_color(tree,\n",
    "                                                           node.tree.node_id)\n",
    "            if node.parent is None:\n",
    "                # root\n",
    "                ax.annotate(node.tree.label, xy, **kwargs)\n",
    "            else:\n",
    "                xy_parent = ((node.parent.x + .5) * scale_x,\n",
    "                             height - (node.parent.y + .5) * scale_y)\n",
    "                kwargs[\"arrowprops\"] = self.arrow_args\n",
    "                ax.annotate(node.tree.label, xy_parent, xy, **kwargs)\n",
    "            for child in node.children:\n",
    "                self.recurse(child, tree, ax, scale_x, scale_y, height,\n",
    "                             depth=depth + 1)\n",
    "\n",
    "        else:\n",
    "            xy_parent = ((node.parent.x + .5) * scale_x,\n",
    "                         height - (node.parent.y + .5) * scale_y)\n",
    "            kwargs[\"arrowprops\"] = self.arrow_args\n",
    "            kwargs['bbox']['fc'] = 'grey'\n",
    "            ax.annotate(\"\\n  (...)  \\n\", xy_parent, xy, **kwargs)\n",
    "\n",
    "\n",
    "def export_graphviz(decision_tree, out_file=None, max_depth=None,\n",
    "                    feature_names=None, class_names=None, label='all',\n",
    "                    filled=False, leaves_parallel=False, impurity=True,\n",
    "                    node_ids=False, proportion=False, rotate=False,\n",
    "                    rounded=False, special_characters=False, precision=3):\n",
    "    \"\"\"Export a decision tree in DOT format.\n",
    "\n",
    "    This function generates a GraphViz representation of the decision tree,\n",
    "    which is then written into `out_file`. Once exported, graphical renderings\n",
    "    can be generated using, for example::\n",
    "\n",
    "        $ dot -Tps tree.dot -o tree.ps      (PostScript format)\n",
    "        $ dot -Tpng tree.dot -o tree.png    (PNG format)\n",
    "\n",
    "    The sample counts that are shown are weighted with any sample_weights that\n",
    "    might be present.\n",
    "\n",
    "    Read more in the :ref:`User Guide <tree>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree : decision tree classifier\n",
    "        The decision tree to be exported to GraphViz.\n",
    "\n",
    "    out_file : file object or string, optional (default=None)\n",
    "        Handle or name of the output file. If ``None``, the result is\n",
    "        returned as a string.\n",
    "\n",
    "        .. versionchanged:: 0.20\n",
    "            Default of out_file changed from \"tree.dot\" to None.\n",
    "\n",
    "    max_depth : int, optional (default=None)\n",
    "        The maximum depth of the representation. If None, the tree is fully\n",
    "        generated.\n",
    "\n",
    "    feature_names : list of strings, optional (default=None)\n",
    "        Names of each of the features.\n",
    "\n",
    "    class_names : list of strings, bool or None, optional (default=None)\n",
    "        Names of each of the target classes in ascending numerical order.\n",
    "        Only relevant for classification and not supported for multi-output.\n",
    "        If ``True``, shows a symbolic representation of the class name.\n",
    "\n",
    "    label : {'all', 'root', 'none'}, optional (default='all')\n",
    "        Whether to show informative labels for impurity, etc.\n",
    "        Options include 'all' to show at every node, 'root' to show only at\n",
    "        the top root node, or 'none' to not show at any node.\n",
    "\n",
    "    filled : bool, optional (default=False)\n",
    "        When set to ``True``, paint nodes to indicate majority class for\n",
    "        classification, extremity of values for regression, or purity of node\n",
    "        for multi-output.\n",
    "\n",
    "    leaves_parallel : bool, optional (default=False)\n",
    "        When set to ``True``, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "    impurity : bool, optional (default=True)\n",
    "        When set to ``True``, show the impurity at each node.\n",
    "\n",
    "    node_ids : bool, optional (default=False)\n",
    "        When set to ``True``, show the ID number on each node.\n",
    "\n",
    "    proportion : bool, optional (default=False)\n",
    "        When set to ``True``, change the display of 'values' and/or 'samples'\n",
    "        to be proportions and percentages respectively.\n",
    "\n",
    "    rotate : bool, optional (default=False)\n",
    "        When set to ``True``, orient tree left to right rather than top-down.\n",
    "\n",
    "    rounded : bool, optional (default=False)\n",
    "        When set to ``True``, draw node boxes with rounded corners and use\n",
    "        Helvetica fonts instead of Times-Roman.\n",
    "\n",
    "    special_characters : bool, optional (default=False)\n",
    "        When set to ``False``, ignore special characters for PostScript\n",
    "        compatibility.\n",
    "\n",
    "    precision : int, optional (default=3)\n",
    "        Number of digits of precision for floating point in the values of\n",
    "        impurity, threshold and value attributes of each node.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dot_data : string\n",
    "        String representation of the input tree in GraphViz dot format.\n",
    "        Only returned if ``out_file`` is None.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn import tree\n",
    "\n",
    "    >>> clf = tree.DecisionTreeClassifier()\n",
    "    >>> iris = load_iris()\n",
    "\n",
    "    >>> clf = clf.fit(iris.data, iris.target)\n",
    "    >>> tree.export_graphviz(clf) # doctest: +ELLIPSIS\n",
    "    'digraph Tree {...\n",
    "    \"\"\"\n",
    "\n",
    "    check_is_fitted(decision_tree, 'tree_')\n",
    "    own_file = False\n",
    "    return_string = False\n",
    "    try:\n",
    "        if isinstance(out_file, str):\n",
    "            out_file = open(out_file, \"w\", encoding=\"utf-8\")\n",
    "            own_file = True\n",
    "\n",
    "        if out_file is None:\n",
    "            return_string = True\n",
    "            out_file = StringIO()\n",
    "\n",
    "        exporter = _DOTTreeExporter(\n",
    "            out_file=out_file, max_depth=max_depth,\n",
    "            feature_names=feature_names, class_names=class_names, label=label,\n",
    "            filled=filled, leaves_parallel=leaves_parallel, impurity=impurity,\n",
    "            node_ids=node_ids, proportion=proportion, rotate=rotate,\n",
    "            rounded=rounded, special_characters=special_characters,\n",
    "            precision=precision)\n",
    "        exporter.export(decision_tree)\n",
    "\n",
    "        if return_string:\n",
    "            return exporter.out_file.getvalue()\n",
    "\n",
    "    finally:\n",
    "        if own_file:\n",
    "            out_file.close()\n",
    "\n",
    "\n",
    "def _compute_depth(tree, node):\n",
    "    \"\"\"\n",
    "    Returns the depth of the subtree rooted in node.\n",
    "    \"\"\"\n",
    "    def compute_depth_(current_node, current_depth,\n",
    "                       children_left, children_right, depths):\n",
    "        depths += [current_depth]\n",
    "        left = children_left[current_node]\n",
    "        right = children_right[current_node]\n",
    "        if left != -1 and right != -1:\n",
    "            compute_depth_(left, current_depth+1,\n",
    "                           children_left, children_right, depths)\n",
    "            compute_depth_(right, current_depth+1,\n",
    "                           children_left, children_right, depths)\n",
    "\n",
    "    depths = []\n",
    "    compute_depth_(node, 1, tree.children_left, tree.children_right, depths)\n",
    "    return max(depths)\n",
    "\n",
    "\n",
    "def export_text(decision_tree, feature_names=None, max_depth=10,\n",
    "                spacing=3, decimals=2, show_weights=False):\n",
    "    \"\"\"Build a text report showing the rules of a decision tree.\n",
    "\n",
    "    Note that backwards compatibility may not be supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decision_tree : object\n",
    "        The decision tree estimator to be exported.\n",
    "        It can be an instance of\n",
    "        DecisionTreeClassifier or DecisionTreeRegressor.\n",
    "\n",
    "    feature_names : list, optional (default=None)\n",
    "        A list of length n_features containing the feature names.\n",
    "        If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n",
    "\n",
    "    max_depth : int, optional (default=10)\n",
    "        Only the first max_depth levels of the tree are exported.\n",
    "        Truncated branches will be marked with \"...\".\n",
    "\n",
    "    spacing : int, optional (default=3)\n",
    "        Number of spaces between edges. The higher it is, the wider the result.\n",
    "\n",
    "    decimals : int, optional (default=2)\n",
    "        Number of decimal digits to display.\n",
    "\n",
    "    show_weights : bool, optional (default=False)\n",
    "        If true the classification weights will be exported on each leaf.\n",
    "        The classification weights are the number of samples each class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    report : string\n",
    "        Text summary of all the rules in the decision tree.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.tree import DecisionTreeClassifier\n",
    "    >>> from sklearn.tree.export import export_text\n",
    "    >>> iris = load_iris()\n",
    "    >>> X = iris['data']\n",
    "    >>> y = iris['target']\n",
    "    >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "    >>> decision_tree = decision_tree.fit(X, y)\n",
    "    >>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "    >>> print(r)\n",
    "    |--- petal width (cm) <= 0.80\n",
    "    |   |--- class: 0\n",
    "    |--- petal width (cm) >  0.80\n",
    "    |   |--- petal width (cm) <= 1.75\n",
    "    |   |   |--- class: 1\n",
    "    |   |--- petal width (cm) >  1.75\n",
    "    |   |   |--- class: 2\n",
    "    ...\n",
    "    \"\"\"\n",
    "    check_is_fitted(decision_tree, 'tree_')\n",
    "    tree_ = decision_tree.tree_\n",
    "    class_names = decision_tree.classes_\n",
    "    right_child_fmt = \"{} {} <= {}\\n\"\n",
    "    left_child_fmt = \"{} {} >  {}\\n\"\n",
    "    truncation_fmt = \"{} {}\\n\"\n",
    "\n",
    "    if max_depth < 0:\n",
    "        raise ValueError(\"max_depth bust be >= 0, given %d\" % max_depth)\n",
    "\n",
    "    if (feature_names is not None and\n",
    "            len(feature_names) != tree_.n_features):\n",
    "        raise ValueError(\"feature_names must contain \"\n",
    "                         \"%d elements, got %d\" % (tree_.n_features,\n",
    "                                                  len(feature_names)))\n",
    "\n",
    "    if spacing <= 0:\n",
    "        raise ValueError(\"spacing must be > 0, given %d\" % spacing)\n",
    "\n",
    "    if decimals < 0:\n",
    "        raise ValueError(\"decimals must be >= 0, given %d\" % decimals)\n",
    "\n",
    "    if isinstance(decision_tree, DecisionTreeClassifier):\n",
    "        value_fmt = \"{}{} weights: {}\\n\"\n",
    "        if not show_weights:\n",
    "            value_fmt = \"{}{}{}\\n\"\n",
    "    else:\n",
    "        value_fmt = \"{}{} value: {}\\n\"\n",
    "\n",
    "    if feature_names:\n",
    "        feature_names_ = [feature_names[i] for i in tree_.feature]\n",
    "    else:\n",
    "        feature_names_ = [\"feature_{}\".format(i) for i in tree_.feature]\n",
    "\n",
    "    export_text.report = \"\"\n",
    "\n",
    "    def _add_leaf(value, class_name, indent):\n",
    "        val = ''\n",
    "        is_classification = isinstance(decision_tree,\n",
    "                                       DecisionTreeClassifier)\n",
    "        if show_weights or not is_classification:\n",
    "            val = [\"{1:.{0}f}, \".format(decimals, v) for v in value]\n",
    "            val = '['+''.join(val)[:-2]+']'\n",
    "        if is_classification:\n",
    "            val += ' class: ' + str(class_name)\n",
    "        export_text.report += value_fmt.format(indent, '', val)\n",
    "\n",
    "    def print_tree_recurse(node, depth):\n",
    "        indent = (\"|\" + (\" \" * spacing)) * depth\n",
    "        indent = indent[:-spacing] + \"-\" * spacing\n",
    "\n",
    "        value = None\n",
    "        if tree_.n_outputs == 1:\n",
    "            value = tree_.value[node][0]\n",
    "        else:\n",
    "            value = tree_.value[node].T[0]\n",
    "        class_name = np.argmax(value)\n",
    "\n",
    "        if (tree_.n_classes[0] != 1 and\n",
    "                tree_.n_outputs == 1):\n",
    "            class_name = class_names[class_name]\n",
    "\n",
    "        if depth <= max_depth+1:\n",
    "            info_fmt = \"\"\n",
    "            info_fmt_left = info_fmt\n",
    "            info_fmt_right = info_fmt\n",
    "\n",
    "            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "                name = feature_names_[node]\n",
    "                threshold = tree_.threshold[node]\n",
    "                threshold = \"{1:.{0}f}\".format(decimals, threshold)\n",
    "                export_text.report += right_child_fmt.format(indent,\n",
    "                                                             name,\n",
    "                                                             threshold)\n",
    "                export_text.report += info_fmt_left\n",
    "                print_tree_recurse(tree_.children_left[node], depth+1)\n",
    "\n",
    "                export_text.report += left_child_fmt.format(indent,\n",
    "                                                            name,\n",
    "                                                            threshold)\n",
    "                export_text.report += info_fmt_right\n",
    "                print_tree_recurse(tree_.children_right[node], depth+1)\n",
    "            else:  # leaf\n",
    "                _add_leaf(value, class_name, indent)\n",
    "        else:\n",
    "            subtree_depth = _compute_depth(tree_, node)\n",
    "            if subtree_depth == 1:\n",
    "                _add_leaf(value, class_name, indent)\n",
    "            else:\n",
    "                trunc_report = 'truncated branch of depth %d' % subtree_depth\n",
    "                export_text.report += truncation_fmt.format(indent,\n",
    "                                                            trunc_report)\n",
    "\n",
    "    print_tree_recurse(0, 1)\n",
    "    return export_text.report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
